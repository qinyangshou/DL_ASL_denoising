{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edsr_hr(\n",
      "  (conv0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  (block_layer): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (resblock_0_conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "      (resblock_0_act): ReLU(inplace=True)\n",
      "      (resblock_0_conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (resblock_1_conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "      (resblock_1_act): ReLU(inplace=True)\n",
      "      (resblock_1_conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (resblock_2_conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "      (resblock_2_act): ReLU(inplace=True)\n",
      "      (resblock_2_conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    )\n",
      "    (3): ResidualBlock(\n",
      "      (resblock_3_conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "      (resblock_3_act): ReLU(inplace=True)\n",
      "      (resblock_3_conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    )\n",
      "    (4): ResidualBlock(\n",
      "      (resblock_4_conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "      (resblock_4_act): ReLU(inplace=True)\n",
      "      (resblock_4_conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    )\n",
      "    (5): ResidualBlock(\n",
      "      (resblock_5_conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "      (resblock_5_act): ReLU(inplace=True)\n",
      "      (resblock_5_conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    )\n",
      "    (6): ResidualBlock(\n",
      "      (resblock_6_conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "      (resblock_6_act): ReLU(inplace=True)\n",
      "      (resblock_6_conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    )\n",
      "    (7): ResidualBlock(\n",
      "      (resblock_7_conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "      (resblock_7_act): ReLU(inplace=True)\n",
      "      (resblock_7_conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    )\n",
      "    (8): ResidualBlock(\n",
      "      (resblock_8_conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "      (resblock_8_act): ReLU(inplace=True)\n",
      "      (resblock_8_conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    )\n",
      "    (9): ResidualBlock(\n",
      "      (resblock_9_conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "      (resblock_9_act): ReLU(inplace=True)\n",
      "      (resblock_9_conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    )\n",
      "    (10): ResidualBlock(\n",
      "      (resblock_10_conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "      (resblock_10_act): ReLU(inplace=True)\n",
      "      (resblock_10_conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    )\n",
      "    (11): ResidualBlock(\n",
      "      (resblock_11_conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "      (resblock_11_act): ReLU(inplace=True)\n",
      "      (resblock_11_conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    )\n",
      "    (12): ResidualBlock(\n",
      "      (resblock_12_conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "      (resblock_12_act): ReLU(inplace=True)\n",
      "      (resblock_12_conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    )\n",
      "    (13): ResidualBlock(\n",
      "      (resblock_13_conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "      (resblock_13_act): ReLU(inplace=True)\n",
      "      (resblock_13_conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    )\n",
      "    (14): ResidualBlock(\n",
      "      (resblock_14_conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "      (resblock_14_act): ReLU(inplace=True)\n",
      "      (resblock_14_conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    )\n",
      "    (15): ResidualBlock(\n",
      "      (resblock_15_conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "      (resblock_15_act): ReLU(inplace=True)\n",
      "      (resblock_15_conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    )\n",
      "    (16): ResidualBlock(\n",
      "      (resblock_16_conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "      (resblock_16_act): ReLU(inplace=True)\n",
      "      (resblock_16_conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    )\n",
      "    (17): ResidualBlock(\n",
      "      (resblock_17_conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "      (resblock_17_act): ReLU(inplace=True)\n",
      "      (resblock_17_conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    )\n",
      "    (18): ResidualBlock(\n",
      "      (resblock_18_conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "      (resblock_18_act): ReLU(inplace=True)\n",
      "      (resblock_18_conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    )\n",
      "    (19): ResidualBlock(\n",
      "      (resblock_19_conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "      (resblock_19_act): ReLU(inplace=True)\n",
      "      (resblock_19_conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    )\n",
      "    (20): ResidualBlock(\n",
      "      (resblock_20_conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "      (resblock_20_act): ReLU(inplace=True)\n",
      "      (resblock_20_conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    )\n",
      "    (21): ResidualBlock(\n",
      "      (resblock_21_conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "      (resblock_21_act): ReLU(inplace=True)\n",
      "      (resblock_21_conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    )\n",
      "    (22): ResidualBlock(\n",
      "      (resblock_22_conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "      (resblock_22_act): ReLU(inplace=True)\n",
      "      (resblock_22_conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    )\n",
      "    (23): ResidualBlock(\n",
      "      (resblock_23_conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "      (resblock_23_act): ReLU(inplace=True)\n",
      "      (resblock_23_conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    )\n",
      "    (24): ResidualBlock(\n",
      "      (resblock_24_conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "      (resblock_24_act): ReLU(inplace=True)\n",
      "      (resblock_24_conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    )\n",
      "    (25): ResidualBlock(\n",
      "      (resblock_25_conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "      (resblock_25_act): ReLU(inplace=True)\n",
      "      (resblock_25_conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    )\n",
      "    (26): ResidualBlock(\n",
      "      (resblock_26_conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "      (resblock_26_act): ReLU(inplace=True)\n",
      "      (resblock_26_conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    )\n",
      "    (27): ResidualBlock(\n",
      "      (resblock_27_conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "      (resblock_27_act): ReLU(inplace=True)\n",
      "      (resblock_27_conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    )\n",
      "    (28): ResidualBlock(\n",
      "      (resblock_28_conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "      (resblock_28_act): ReLU(inplace=True)\n",
      "      (resblock_28_conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    )\n",
      "    (29): ResidualBlock(\n",
      "      (resblock_29_conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "      (resblock_29_act): ReLU(inplace=True)\n",
      "      (resblock_29_conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    )\n",
      "    (30): ResidualBlock(\n",
      "      (resblock_30_conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "      (resblock_30_act): ReLU(inplace=True)\n",
      "      (resblock_30_conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    )\n",
      "    (31): ResidualBlock(\n",
      "      (resblock_31_conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "      (resblock_31_act): ReLU(inplace=True)\n",
      "      (resblock_31_conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    )\n",
      "  )\n",
      "  (conv_penultimate): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  (conv_final): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from unetr import UNETR\n",
    "from edsr import edsr_hr\n",
    "from torchviz import make_dot\n",
    "import torch\n",
    "import numpy as np\n",
    "# define the input shape\n",
    "#X = torch.tensor(np.zeros((5,1,32,32,32))).double()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNETR(\n",
      "  (transformer): Transformer(\n",
      "    (embeddings): Embeddings(\n",
      "      (patch_embeddings): Conv3d(1, 768, kernel_size=(16, 16, 16), stride=(16, 16, 16))\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (layer): ModuleList(\n",
      "      (0): TransformerBlock(\n",
      "        (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=768, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (attn): SelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (proj_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (1): TransformerBlock(\n",
      "        (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=768, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (attn): SelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (proj_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (2): TransformerBlock(\n",
      "        (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=768, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (attn): SelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (proj_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (3): TransformerBlock(\n",
      "        (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=768, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (attn): SelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (proj_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (4): TransformerBlock(\n",
      "        (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=768, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (attn): SelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (proj_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (5): TransformerBlock(\n",
      "        (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=768, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (attn): SelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (proj_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (6): TransformerBlock(\n",
      "        (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=768, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (attn): SelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (proj_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (7): TransformerBlock(\n",
      "        (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=768, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (attn): SelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (proj_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (8): TransformerBlock(\n",
      "        (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=768, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (attn): SelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (proj_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (9): TransformerBlock(\n",
      "        (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=768, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (attn): SelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (proj_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (10): TransformerBlock(\n",
      "        (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=768, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (attn): SelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (proj_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (11): TransformerBlock(\n",
      "        (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=768, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (attn): SelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (proj_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder0): Sequential(\n",
      "    (0): Conv3DBlock(\n",
      "      (block): Sequential(\n",
      "        (0): SingleConv3DBlock(\n",
      "          (block): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        )\n",
      "        (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Conv3DBlock(\n",
      "      (block): Sequential(\n",
      "        (0): SingleConv3DBlock(\n",
      "          (block): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        )\n",
      "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder3): Sequential(\n",
      "    (0): Deconv3DBlock(\n",
      "      (block): Sequential(\n",
      "        (0): SingleDeconv3DBlock(\n",
      "          (block): ConvTranspose3d(768, 512, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "        )\n",
      "        (1): SingleConv3DBlock(\n",
      "          (block): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        )\n",
      "        (2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Deconv3DBlock(\n",
      "      (block): Sequential(\n",
      "        (0): SingleDeconv3DBlock(\n",
      "          (block): ConvTranspose3d(512, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "        )\n",
      "        (1): SingleConv3DBlock(\n",
      "          (block): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        )\n",
      "        (2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Deconv3DBlock(\n",
      "      (block): Sequential(\n",
      "        (0): SingleDeconv3DBlock(\n",
      "          (block): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "        )\n",
      "        (1): SingleConv3DBlock(\n",
      "          (block): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        )\n",
      "        (2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder6): Sequential(\n",
      "    (0): Deconv3DBlock(\n",
      "      (block): Sequential(\n",
      "        (0): SingleDeconv3DBlock(\n",
      "          (block): ConvTranspose3d(768, 512, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "        )\n",
      "        (1): SingleConv3DBlock(\n",
      "          (block): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        )\n",
      "        (2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Deconv3DBlock(\n",
      "      (block): Sequential(\n",
      "        (0): SingleDeconv3DBlock(\n",
      "          (block): ConvTranspose3d(512, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "        )\n",
      "        (1): SingleConv3DBlock(\n",
      "          (block): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        )\n",
      "        (2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder9): Deconv3DBlock(\n",
      "    (block): Sequential(\n",
      "      (0): SingleDeconv3DBlock(\n",
      "        (block): ConvTranspose3d(768, 512, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "      )\n",
      "      (1): SingleConv3DBlock(\n",
      "        (block): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "      )\n",
      "      (2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (decoder12_upsampler): SingleDeconv3DBlock(\n",
      "    (block): ConvTranspose3d(768, 512, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "  )\n",
      "  (decoder9_upsampler): Sequential(\n",
      "    (0): Conv3DBlock(\n",
      "      (block): Sequential(\n",
      "        (0): SingleConv3DBlock(\n",
      "          (block): Conv3d(1024, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        )\n",
      "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Conv3DBlock(\n",
      "      (block): Sequential(\n",
      "        (0): SingleConv3DBlock(\n",
      "          (block): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        )\n",
      "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Conv3DBlock(\n",
      "      (block): Sequential(\n",
      "        (0): SingleConv3DBlock(\n",
      "          (block): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        )\n",
      "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (3): SingleDeconv3DBlock(\n",
      "      (block): ConvTranspose3d(512, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "    )\n",
      "  )\n",
      "  (decoder6_upsampler): Sequential(\n",
      "    (0): Conv3DBlock(\n",
      "      (block): Sequential(\n",
      "        (0): SingleConv3DBlock(\n",
      "          (block): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        )\n",
      "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Conv3DBlock(\n",
      "      (block): Sequential(\n",
      "        (0): SingleConv3DBlock(\n",
      "          (block): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        )\n",
      "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): SingleDeconv3DBlock(\n",
      "      (block): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "    )\n",
      "  )\n",
      "  (decoder3_upsampler): Sequential(\n",
      "    (0): Conv3DBlock(\n",
      "      (block): Sequential(\n",
      "        (0): SingleConv3DBlock(\n",
      "          (block): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        )\n",
      "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Conv3DBlock(\n",
      "      (block): Sequential(\n",
      "        (0): SingleConv3DBlock(\n",
      "          (block): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        )\n",
      "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): SingleDeconv3DBlock(\n",
      "      (block): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "    )\n",
      "  )\n",
      "  (decoder0_header): Sequential(\n",
      "    (0): Conv3DBlock(\n",
      "      (block): Sequential(\n",
      "        (0): SingleConv3DBlock(\n",
      "          (block): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        )\n",
      "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Conv3DBlock(\n",
      "      (block): Sequential(\n",
      "        (0): SingleConv3DBlock(\n",
      "          (block): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        )\n",
      "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): SingleConv3DBlock(\n",
      "      (block): Conv3d(64, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "my_UNETR = UNETR(input_dim=1, output_dim=1, img_shape=(32,32,32)).double()\n",
    "#y = my_UNETR(X)\n",
    "#make_dot(y, params=dict(my_edsr.named_parameters())).render(\"EDSR_torchviz\", format=\"png\")\n",
    "print(my_UNETR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
